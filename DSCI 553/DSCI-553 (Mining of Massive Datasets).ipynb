{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni's Principle\n",
    "\n",
    "Suppose you have a certain amount of data, and you look for events of a certain type within that data. You can expect events of this type to occur, even if the data is completely random, and the number of occurrences of these events will grow as the size of the data grows. These occurrences are “bogus,” in the sense that they have no cause other than that random data will always have some number of unusual features that look significant but aren’t.\n",
    "\n",
    "\n",
    "A theorem of statistics, known as the **Bonferroni correction** gives a statistically sound way to avoid most of these bogus positive responses to a search through the data. Without going into the statistical details, we offer an informal version, Bonferroni’s principle, that helps us avoid treating random occurrences as if they were real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Scale File System organization\n",
    "\n",
    "This new file system is called distributed file system (DFS) and is used for:\n",
    "    \n",
    "    - Enormous files, possibly in terrabyte in size. For, small files have no point to use DFS \n",
    "    - Files are rarely updated. They are used to read data or appending additional data.\n",
    "    \n",
    "\n",
    "Files are divided into chunks (typically 64 MB). Chunks are replicated (~3 times) on different compute nodes (should be different racks). To find the chunk of file, there is another small file master node or name node for that file. Master node is itself replicated.\n",
    "\n",
    "\n",
    "**Chunk Servers**\n",
    "1. When a map reduce job is started, chunks are distributed among different worker nodes/ chunk servers with some replication factor\n",
    "2. Each chunk server can have multiple chunks of data, with each chunk having its own mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce\n",
    "\n",
    "* Manages large scale computations in a way that is tolerant of hardware faults.\n",
    "* 2 Functions, Map and Reduce, while system manages the parallel execution.\n",
    "\n",
    "**Steps for map-reduce computation**\n",
    "1. Some number of Map tasks each are given one or more chunks from a distributed file system. These map tasks turns the chunk into a sequence of key-value pairs.\n",
    "2. Key-Value pairs from each map task are collected by a master controller and sorted by key. The key are divided among all the reduce tasks. All key-value pairs with same key wind up at same Reduce task.\n",
    "3. The Reduce tasks work on one key at a time and combine all the values associated with the key in some way.\n",
    "\n",
    "\n",
    "* **Map Task**\n",
    "* **Grouping and Aggregation**\n",
    "* **Reduce Task**\n",
    "        \n",
    "**Coping with Failures**\n",
    "1. If node at with Master is executing fails, then the entire map-reduce job must be restarted. This is the worst case, other failures will be managed by the Master, and map-reduce job will complete eventually.\n",
    "2. Suppose if Map worker fails. The fail will be detected by Master because it periodically pings the Workers. All map task assigned to this worker will have to redone even if it is completed. Becuase output is destined to Reduce tasks, and now unavailable to reduce tasks. The master must also inform reduce tasks that the location of input from map task has changed.\n",
    "3. Dealing with failure at the node of Reduce worker is simpler. The Master simply sets the status of its currently executing Reduce tasks to idle. These will be rescheduled on another reduce worker later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow\n",
    "1. Input and output data is stored on DFS. Scheduler tries to map tasks \"close\" to physical storage (chunk servers) of input data.\n",
    "2. Intermediate results are stored on local FS of Map and reduce workers\n",
    "3. Output is often input to another MapReduce task.\n",
    "\n",
    "### Coordination : Master\n",
    "1. Check status of task (idle, in-progress, completed)\n",
    "2. idle - no worker assigned, gets scheduled as worker become available\n",
    "3. When map tasks completes, it sends the master the location/sizes of intermediate files, one for each reducer.\n",
    "4. Master pings workers periodically to detect failures.\n",
    "\n",
    "**Task Granularity :** Measures amount of work which is performed by task.\n",
    "\n",
    "**Fine Granularity tasks :** Finer grains -> more parallelism -> speedup. Minimizes time for fault recovery, pipeline shuffling, better load balancing.\n",
    "\n",
    "**Refinement (Backup tasks) :** Slow worker -> increases job completion time. **Solution** is to keep backup copies of tasks. Whichever finishes first \"wins\". **Effect** Dramatically shortens job completion time.\n",
    "\n",
    "**Refinement (Combiners) :** It is common for reduce function to be associate and commulative. So, it is possible to push some of what reduce does to Map tasks. Example instead of emitting (w,1) we could apply reduce function within the Map tasks, before it is subjected to grouping.\n",
    "\n",
    "**Refinement (partition) :** Controlling how keys get partitioned, override the default partition function. Reducer need to ensure that record with same intermediate key end up at the same worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms using Map-Reduce\n",
    "\n",
    "Map-Reduce is not a solution to every problem, not even every problem that profitable can use many compute nodes operating in parellel.\n",
    "\n",
    "1. **Matrix-Vector Multiplication**\n",
    "   We have Matrix M (n x n) where $m_{ij}$ denote element at row i and column j. We also have a vector v of length n, jth element is $v_j$. Element $x_i$ is given by.\n",
    "$$ x_i = \\sum_{j=1}^n m_{ij}v_j $$\n",
    "    * The matrix will be stored in DFS in the form of triples (i,j, $m_{ij}$).\n",
    "    * *Map Function*: Each Map task will take entire vector v and a chunk of matrix M, to produce a key value pair. (i, $m_{ij}v_j$).\n",
    "    * *Reduce Function*: A reduce task has simply to sum all values associated with a given key i. The result will be pair (i,$x_i$).\n",
    "\n",
    "2. **Computing Selections**\n",
    "    Do not need full power of map-reduce. Can be done in map portion alone.\n",
    "    * *Map Function*: For each tuple t in R test if it satisfies C. If so, produce the key-value pair (t,t).\n",
    "    * *Reduce Function*: Simply passes each key-value pair to the output.\n",
    "\n",
    "3. **Computing Projections**\n",
    "    Similar to selection, because may cause duplicates, reduce function must eliminate duplicates.\n",
    "    * *Map Function*: For each tuple t in R, construct t'. Output the key-value pair (t', t').\n",
    "    * *Reduce Function*: For each key t' just output one t'.\n",
    "\n",
    "\n",
    "4. **Union**\n",
    "    * *Map Function*: Turn each input tuple t into a key-value pair (t,t).\n",
    "    * *Reduce Function*: Associated with each key t there will be either one/two values. Produce output (t,t) in either case.\n",
    "\n",
    "\n",
    "5. **Intersection**:\n",
    "    * *Map Function*: Turn each tuple t into a key-value pair (t,t).\n",
    "    * *Reduce Function*: If key t has value list [t,t], then produce (t,t). Other wise produce (t, NULL)\n",
    "\n",
    "\n",
    "6. **Difference**:\n",
    "    * *Map Function*: For a tuple t in R, produce key-value pair (t,R) or (t,S).\n",
    "    * *Reduce Function*: If assoicated value list is [R], then produce (t,t). else produce (t,NULL).\n",
    "\n",
    "7. **Natural Join**\n",
    "    * *Map Function*: For each tuple (a,b) of R, produce (b, (R,a)) as key-value pair. Similarly for S.\n",
    "    * *Reduce Function*: Each key value b will be associated with a list of pairs that are either of the form (R,a) or (S,c). The outptut will be (b, (a,b,c)).\n",
    "    \n",
    "8. **Two Phase Map Reduce Matrix Multiply**\n",
    "    * *1st Map Function*\n",
    "\n",
    "     $$ A[ij] = emit( j, (A, i, A[ij]) ) $$ $$ B[jk] = emit( j, (B, k, B[jk] ) ) $$\n",
    "    \n",
    "    * *1st Reduce Function*\n",
    "     \n",
    "     For each (i,k) from A and B i.e (A, i, A[ij]) and (B, k, B[jk])\n",
    "    \n",
    "     $$ emit( (i,k), A[ij]*B[jk] ) $$\n",
    "     \n",
    "    * *2nd Map Function*\n",
    "     \n",
    "     pass through, does nothing\n",
    "     $$ emit( k,v ) $$\n",
    "     \n",
    "    * *2nd Reduce Function*\n",
    "     \n",
    "     $$ emit( (i,k), sum(values) ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Model\n",
    "\n",
    "* Goal : Identify items that are brought together by **sufficiently many customers**\n",
    "* Each basket is a small subset of items.\n",
    "* I/o : List of baskets\n",
    "* O/p : Association Rules Discovered (One Way)\n",
    "* General many-many mappings between items(not baskets)\n",
    "* Focuses on COMMON EVENTS (not rare ones)\n",
    "\n",
    "### Support\n",
    "* Support for Itemset I : Number of baskets containing all items in I (%)\n",
    "* If support > support threshold: Itemset i is frequent itemset\n",
    "\n",
    "        Conf(I -> j) = Support(I U j) / Support(I)\n",
    "\n",
    "* Interesting Association Rules\n",
    "\n",
    "        Not all high confidence rules are interesting\n",
    "\n",
    "        Interest(I->j) = | Conf(I->j) - Pr[j] |\n",
    "        \n",
    "High positive or negative interest means presence of I encourages/discourages presence of j\n",
    "\n",
    "\n",
    "**Association rules which have support >= s and confidence >= c will be frequent!**\n",
    "\n",
    "### Computational Model\n",
    "\n",
    "* MB Data kept in flat files on disk (not in db)\n",
    "* True cost of mining is number of disk I/O.\n",
    "* Association Rules read  data in passes which is used as cost measure\n",
    "* Main Memory Bottleneck : Counting the pairs\n",
    "\n",
    "**Naive Approach :** Number of pairs ina basket = nC2 = n!/r! (n-r)!\n",
    "(Requires too much main memory)\n",
    "\n",
    "> Approach 1:\n",
    "* Triangular Matrix : \n",
    "        \n",
    "        Pair Count at position (i,j) = (i-1)(n-i/2) + j - i\n",
    "        Every time you see a pair (i,j) from basket, increment counter at that position.\n",
    "        4 bytes / pair\n",
    "\n",
    "*Approach 2:*\n",
    "\n",
    "\n",
    "### Apriori Algorithm\n",
    "> 2 pass approach : \n",
    "        \n",
    "        Pass 1 - Identifies frequent itemsets of size 1\n",
    "        Pass 2 - Identifies frequent itemsets of size 2 for pairs frequent in pass 1\n",
    "\n",
    "> Limits need of main memory\n",
    "\n",
    "> Monotonicity : If set of items I appears s tines, so does every subset J of I\n",
    "\n",
    "> Contrapositive for Pairs : If item i doesn't appear in s baskets, no pair including i can appear in s baskets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Park-Chen-Yu Algorithm\n",
    "\n",
    "1. During Pass 1 of A-priori, most memory is idle.\n",
    "2. Use that memory to keep counts of buckets into which pairs of items are hashed.\n",
    "3. For each bucket just keep the count, not the actual pairs that were used to create that bucket.\n",
    "4. \n",
    "\n",
    "        For each basket\n",
    "            for each item in basket\n",
    "                add 1 to item's count\n",
    "            \n",
    "            #PCY\n",
    "            for each pair of item\n",
    "                hash the pair to a bucket\n",
    "                add 1 to count for that bucket\n",
    "                \n",
    "5. Pairs of items need to be generated from input file, they are not present in file\n",
    "6. A bucker is frequent if its count is at least the support threshold s.\n",
    "7. If a bucket contains a frequent pair then the bucket is surely frqueunt.\n",
    "8. However even without a frequent pair, a bucket  can be still be frquent. (So we cant use hash to eliminate any member of a frequent bucket.)\n",
    "9. But for abucket with total count less that s, none of its pairs can be frequent. (pairs that hash to this bucket can be eliminated as candidates)\n",
    "\n",
    "**Between Passes**\n",
    "1. Replace the buckets by a bit-vector. 1 meaning bucket count exceeded the support s (frequent bucket)\n",
    "2. 4 bytes (32 bits) integer counts are replaced by bits, so the bit-vector requires 1/32 of memory.\n",
    "\n",
    "**Pass 2**:\n",
    "1. Count all pairs {i,j} that meet condition for being a candidate pair:\n",
    "    \n",
    "       Both i and j are frequeunt items.\n",
    "       The pairs {i,j} hashes to a bucket whose bit in the bit vector is 1 (frquent bucket)\n",
    "       \n",
    "       Both conditions are necessary for the pair to have a chance of being frequent.\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "1. Cannot use triangular matrix approach.\n",
    "2. To use PCY, hash table must eliminate approx 2/3 of the candidates pairs for PCY to beat A-priori.\n",
    "\n",
    "\n",
    "**Refinement: Multistage Algorithm**\n",
    "1. Limit the numbers of candidates to be counted.\n",
    "2. Still need to generate all itemssets but we only want to count/keep track of the ones that are frequent.\n",
    "3. **Key idea**: After Pass 1 of PCY, rehash only those pairs that qualify for pass 2 of PCY.\n",
    "4. \n",
    "\n",
    "        Pass 1: Count items Hash pairs {i,j}\n",
    "        Pass 2: Hash pairs {i,j} into Hash2 iff:\n",
    "            i,j are frequent, {i,j} hashes to be freq in bucket B1.\n",
    "        Pass 3: Count pairs {i,j} iff i,j are frequeny {i,j} hashes to freq. bucket in B2 {i,j} hashes to freq. bucket in B2.\n",
    "        \n",
    "        \n",
    "**Refinement: Multihash**\n",
    "1. **Key Idea** : Use several independent hash tables on the first pass.\n",
    "2. **Risk**: Having number of buckets doubles the average count. \n",
    "        \n",
    "        We have to be sure most buckets will still not reach count s.\n",
    "        \n",
    "3. If so, we can get a benefit like multistage, but in only 2 passes.\n",
    "\n",
    "4. In second pass, it will be same as multistage, Count only those pairs {i,j} that satisfy these candidates pair condition:\n",
    "    \n",
    "       - Both i and j are frequent items\n",
    "       - Using 1st hash function, the pair hashes to a bucket whose bit-vector is 1\n",
    "       - Using 2nd hash function the pair hashes to a bucket whose bit in the second bit-vector is 1.\n",
    "       \n",
    "**PCY extensions**\n",
    "1. In multistage, there is a point of diminishing return, since bit-vectors eventually consume all main memory.\n",
    "2. For multihash, the bit-vectors occupy exactly what one PCY bitmap does, but too many hash functions makes all count >= s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited Pass Algorithm\n",
    "\n",
    "* So far, compute exact collection of frequent itemsets of size k in k passes. (A-priori, PCY, Multistage, Multihash)\n",
    "\n",
    "* In Many applications it is not essential to discover every frequent itemset, it is sufficient to discover most of them.\n",
    "\n",
    "* Algorithms that find all or most frequent itemsets using at most 2 passes over data\n",
    "\n",
    "    - Sampling\n",
    "    - SON\n",
    "    - Toivonen's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling\n",
    "\n",
    "1. Take a random sample of the market baskets that fits in the main memory. (Leave enough space in memory for counts)\n",
    "2. Run a-priori or one of its improvement in main memory.\n",
    "    - For sets of all sizes, not just pairs\n",
    "    - Don't pay for disk I/O each time we increase the size of itemsets\n",
    "    - Reduce support threshold proportionally to match the sample size.\n",
    "3. In Random Sampling we cannot guarantee:\n",
    "    - Algorithm will produce all itemsets that are frequent in whole dataset. (False negative)\n",
    "    - Produce only itemsets that are frequent in whole datasets (False positive)\n",
    "4. Avoid Errors\n",
    "    - Eliminate False positives: Count/validate them in entire dataset once found.\n",
    "    - Reduce False negative: Reduce the threshold like e.g. 0.9ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savasere, Omiecinski and Navathe (SON) algorithm\n",
    "\n",
    "1. Avoids false negative and false positives.\n",
    "2. Requires two full passes over data.\n",
    "3. 1st Pass\n",
    "\n",
    "       Repeatedly read small subsets of the baskets into main memory. (it is not sampling, we will process entire file in memory-sized chunks)\n",
    "       Run an in-memory algorithm (eg. a priori, random sampling, PCY) to find all frequent itemsets.\n",
    "       An itemset becomes a candidate if it is found to be frequent in any one or more subsets of the baskets.\n",
    "       \n",
    "4. 2nd Pass\n",
    "    \n",
    "       Count all the candidate itemsets and determine which are frequent in the entire set.\n",
    "       \n",
    "       Key \"monotonicity\" idea: an itemset cannot be frequent in the entire set of baskets unless it is frequent in at least one subset.\n",
    "       \n",
    "5. Distributed Version:\n",
    "    \n",
    "* SON lends iteself to distributed data mining (Map Reduce)\n",
    "* Baskets distibuted among many nodes\n",
    "    - Subsets of data may correspond to one or more chunks in DFS.\n",
    "    - Compute frequent itemsets at each node.\n",
    "    - Distribute candidates to all nodes\n",
    "    - Accumulate the count of all candidates\n",
    "* **Phase 1**: Find candidate itemset\n",
    "    - Map:\n",
    "            \n",
    "            * Input is a chunk/subset of all baskets; fraction p of total input file.\n",
    "            * Find itemsets frequent in that subset (eg. use Random Sampling algo)\n",
    "            * Use support threshold ps\n",
    "            * Output is set of key-value pairs (F,1) where F is frequent itemsets from sample.\n",
    "    \n",
    "    - Reduce:\n",
    "    \n",
    "            * Each reduce task is assigned set of keys, which are itemsets\n",
    "            * Produce keys that appear one or more time.\n",
    "            * Frequent in some subset\n",
    "            * These are candiate itemsets.\n",
    "    \n",
    "* **Phase 2**: Find true frequent itemsets\n",
    "    - Map\n",
    "    \n",
    "            * Each Map task takes output from 1st Reduce task AND a chunk of total input data file.\n",
    "            * All candidate itemsets go to every Map task.\n",
    "            * Count occurences of each candidate itemsets amoung the baskets in input chunk.\n",
    "            * Output is set of key-value pair (C,v) where C is candidate frequent itemsets and v is support for that itemset among the baskets in the input chunk.\n",
    "            \n",
    "    - Reduce\n",
    "    \n",
    "            *  Each reduce tasks is assigned a set of keys (itemsets)\n",
    "            * Sums associated values for each key: total support for itemset\n",
    "            * If support of itemset >= s, emit itemset and its count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toivonen's Algorithm\n",
    "\n",
    "    * Another Approximation algorithm\n",
    "    * Given sufficient main memory, uses one pass over a small sample and one full pass over data.\n",
    "    * Gives no false positives or false negatives.\n",
    "    * But there is small but finite probability it will fail to produce an answer (Will not identify frquent itemsets)\n",
    "    * If fails must be repeated with different sample untill gives an answer.\n",
    "\n",
    "> First find candidates frequent itemsets from sample\n",
    "    - Start as in random sampling algorithm, but lower the threshold slightly for the sample. (0.8ps)\n",
    "    - Goal is to avoid missing any itemset that is frequent in the full set of baskets.\n",
    "    - The smaller the threshold: The more memory is needed to count all candidates itemsets, less likely the algorithm will not find an answer. \n",
    "\n",
    "* After finding frequent itemsets for the sample, construct  the negative border.\n",
    "    \n",
    "    * Negative Border: Collection of itemsets that are not frequent in the sample but all of their immediate subsets are frequent.\n",
    "    * Immediate subset is constructed by deleting exactly one item.\n",
    "    * Example: ABCD is neagtive border if and only if\n",
    "        1. It is not frequent in sample, but\n",
    "        2. All of ABC, BCD, ACD and ABD are frequent\n",
    "\n",
    "* **First Pass**\n",
    "    * first find candidate frequent itemsets from sample\n",
    "    * Identify itemsets that are frequent for the sample\n",
    "    * Construct the negative border.\n",
    "    * Itemsets that are not frequent in the sample but all their immediate subsets are frequent.\n",
    "\n",
    "* **Second Pass**\n",
    "    * In second pass, process the whole file (no sampling)\n",
    "    * Count all candiate frequent itemset from first pass and for all itemsets on the negative border\n",
    "    \n",
    "* **Case 1:** No itemset from negative border turns out to be frequent in the whole data set. (Correct set of frequent itemsets is exactly the itemsets from the sample that were found frequent in the whole data)\n",
    "\n",
    "* **Case 2:** Some member of negative border is frequent in the whole dataset. Can give no answer at this time. Must repeat the algorithm with new random sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
