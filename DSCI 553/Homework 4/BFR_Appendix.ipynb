{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "import random\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "import math\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "files = list(map(lambda x:\"data/test1/\"+x, os.listdir('data/test1')))\n",
    "files.sort( key=lambda x: os.path.getsize(x), reverse=True )\n",
    "ans = dict()\n",
    "alpha = 3\n",
    "discard_set = dict()\n",
    "compression_set = dict()\n",
    "retained_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoad(file):\n",
    "    data = []\n",
    "    fp = open(file, \"r\")\n",
    "    for line in fp.readlines():\n",
    "        points = line.split(',')\n",
    "        data.append([float(x) for x in points])\n",
    "    fp.close()\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def euclidean(x1, x2):\n",
    "    ans = 0\n",
    "    for i1, i2 in zip(x1,x2):\n",
    "        ans += (i1-i2)**2\n",
    "    return math.sqrt(ans)\n",
    "\n",
    "def argmin(x):\n",
    "    n = len(x)\n",
    "    min_x, min_i = float('inf'), 0\n",
    "    for i, xi in zip(range(n), x):\n",
    "        if xi < min_x: min_x, min_i = xi, i\n",
    "    return min_i\n",
    "\n",
    "def add_vector( v1, v2 ):\n",
    "    v = []\n",
    "    for i,j in zip(v1,v2): v.append(i+j)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"data/test2/\"\n",
    "n_cluster = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3\n",
    "files = list(map(lambda x: input_dir+x, os.listdir(input_dir)))\n",
    "files.sort( key=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_dict = {}\n",
    "discard_set = {}\n",
    "compression_set = {}\n",
    "retained_set = {}\n",
    "\n",
    "intermedites = [\"round_id,nof_cluster_discard,nof_point_discard,nof_cluster_compression,nof_point_compression,nof_point_retained\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(x1, x2):\n",
    "    ans = 0\n",
    "    for i1, i2 in zip(x1,x2):\n",
    "        ans += (i1-i2)**2\n",
    "    return math.sqrt(ans)\n",
    "\n",
    "def add_vector( v1, v2 ): return [ a+b for a,b in zip(v1, v2) ]\n",
    "\n",
    "def argmin(x):\n",
    "    n = len(x)\n",
    "    min_x, min_i = float('inf'), 0\n",
    "    for i, xi in zip(range(n), x):\n",
    "        if xi < min_x: min_x, min_i = xi, i\n",
    "    return min_i\n",
    "\n",
    "def getCentroid(N, SUM): return [ a/N for a in SUM ]\n",
    "def getStd(N, SUM, SUMSQ): return [ ((sq/N)-(s/N)**2)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getDataLoad(files[0])\n",
    "d = len(data[0])-1\n",
    "threshold = 3*math.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, n_clusters=10, max_iterations=float('inf')):\n",
    "        self.k = n_clusters\n",
    "        self.max_it = max_iterations\n",
    "    \n",
    "    def cluster_changed(self, old, new):\n",
    "        for o,n in zip(old, new):\n",
    "            if o!=n: return True\n",
    "        return False\n",
    "    \n",
    "    def initialize_cluster(self, x):\n",
    "        return random.sample(x, self.k)\n",
    "    \n",
    "    def initialize_cluster_later(self, x, n_clusters):\n",
    "        cluster_centers = [random.sample( x, 1 )[0]]\n",
    "        for i in range(1, n_clusters):\n",
    "            dist, idx = 0, 0\n",
    "            for j in range(len(x)):\n",
    "                curr = 0\n",
    "                for k in range(i):\n",
    "                    curr += euclidean( x[j], cluster_centers[k]  )\n",
    "                if curr > dist: dist, idx = curr, j\n",
    "            cluster_centers.append(x[idx])\n",
    "        return cluster_centers\n",
    "\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"returns ans, sumamry, Map[id, cluster_id], \"\n",
    "        initial = sc.parallelize(data).map(lambda x: ( str(int(x[0])), x[1:] ))\n",
    "        cluster_centers = self.initialize_cluster([row[1:] for row in data])\n",
    "        i = 0\n",
    "        while i != self.max_it:\n",
    "            point_cluster = initial.mapValues( lambda x: [euclidean(x, center) for center in cluster_centers] ) \\\n",
    "                             .mapValues(lambda x: argmin(x)).collectAsMap() #(id, cluster_id)\n",
    "\n",
    "            new_cluster_centers = initial.map(lambda x: (point_cluster[x[0]], (x[1],1)) ) \\\n",
    "                            .reduceByKey( lambda x,y: (add_vector(x[0],y[0]), x[1]+y[1]) ) \\\n",
    "                            .mapValues( lambda x: [y/x[1] for y in x[0]] )\n",
    "\n",
    "            new_cluster_centers = [ x[1] for x in sorted(new_cluster_centers.collect()) ]\n",
    "\n",
    "            if self.cluster_changed(cluster_centers, new_cluster_centers):\n",
    "                cluster_centers = new_cluster_centers\n",
    "            else: \n",
    "                cluster_centers = new_cluster_centers \n",
    "                break\n",
    "            i+=1\n",
    "        summary = initial.mapValues( lambda x: ([euclidean(x, center) for center in cluster_centers], x) ) \\\n",
    "                        .map( lambda x: (argmin(x[1][0]), (1, x[1][1], [v**2 for v in x[1][1]])) ) \\\n",
    "                        .reduceByKey( lambda x,y: [x[0]+y[0], add_vector(x[1],y[1]), add_vector(x[2],y[2])] ) \\\n",
    "                        .collectAsMap() \n",
    "        \n",
    "        cluster_points = initial.map( \n",
    "            lambda x: (argmin([euclidean(x[1], center) for center in cluster_centers]), (x[0], x[1]) ) ) \\\n",
    "            .groupByKey().mapValues(list).collectAsMap()\n",
    "        \n",
    "        return point_cluster, summary, cluster_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_retained( ans, summary,  cluster_points):\n",
    "    retained = []\n",
    "    for k,v in list(summary.items()):\n",
    "        if v[0] <= 1:\n",
    "            if v[0] == 1:\n",
    "                key, val = cluster_points[k][0]\n",
    "                retained.append(  [float(key)]+val ) \n",
    "                ans.pop(key)\n",
    "            summary.pop(k)\n",
    "            cluster_points.pop(k)\n",
    "    return ans, summary, cluster_points, retained\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6395, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction = min(30000, int(len(data)*0.3))\n",
    "sample = data[:fraction]\n",
    "\n",
    "ds_ans, ds_summary, _ = KMeans(n_clusters = n_clusters).fit(sample)\n",
    "len(ds_ans), len(ds_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14918, 25, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_data = data[fraction:]\n",
    "point_cluster, summary, cluster_points = KMeans(n_clusters = n_clusters*3, max_iterations = 10).fit(rest_data)\n",
    "\n",
    "cs_map, cs_summary, _, retain = seperate_retained(point_cluster, summary, cluster_points)\n",
    "retained_set.extend(retain)\n",
    "len(cs_map), len(cs_summary), len(retained_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance( point, N, SUM, SUMSQ ):\n",
    "    mh = 0\n",
    "    for i in range(d):\n",
    "        std = (SUMSQ[i]/N) - (SUM[i]/N)**2\n",
    "        centroid = SUM[i]/N\n",
    "        if std==0: normalized = point[i]-centroid\n",
    "        else: normalized = (point[i]-centroid)/std\n",
    "        mh += (normalized**2)\n",
    "    return math.sqrt(mh)\n",
    "\n",
    "\n",
    "def assign_to_cluster( point, threshold, summary ):\n",
    "    min_idx, min_mh = 0, float('inf')\n",
    "    \n",
    "    for idx, summ in summary.items():\n",
    "        N, SUM, SUMSQ = summ[0], summ[1], summ[2]\n",
    "        mh = mahalanobis_distance(point, N, SUM, SUMSQ)\n",
    "        if mh < min_mh: min_mh, min_idx = mh, idx\n",
    "            \n",
    "    if min_mh < threshold:\n",
    "        return min_idx\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def updateSummary(old_sum, updates):\n",
    "    for idx, summary in updates.items():\n",
    "        old_sum[idx][0] += summary[0]\n",
    "        for i in range(d):\n",
    "            old_sum[idx][1][i] += summary[1][i]\n",
    "            old_sum[idx][2][i] += summary[2][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getDataLoad(files[1])\n",
    "rdd = sc.parallelize(data).map( lambda x: (str(int(x[0])), x[1:]) ) \\\n",
    "        .map(lambda x: (assign_to_cluster(x[1], threshold, ds_summary), x[0], x[1] ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
