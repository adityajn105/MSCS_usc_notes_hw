Complete this document and upload along with your prediction results and your code.

### Method Name ###
Use a phrasal name to describe your method, e.g. training a BiLSTM cross-encoder from scratch, fine-tuning RoBERTa-large-MNLI, etc.

Answer: Fine-Tuning NLI-Deberta-v3-base


### Sentence pair encoder ###
Use up to 5 sentences to describe your encoder for the sentence pairs. Need to mention the following:
- Is it a bi-encoder or cross-encoder?
- What type of encoder (LSTM, Transformer, etc.)
- Is it based on a pre-trained model (BERT-large? RoBERTa-large-SNLI? BART-large-MNLI?) or completely trained from scratch by yourself (then how do you chracterize the words and aggregate them into sentence representations)?

Answer:
NLI-Deberta-v3 is a cross encoder which uses something unique called disentangled attention mechanism which as a result performs consistently better on multiple NLP tasks. It is transformer based model. It is a pre-trained model which has been fine-tuned on Stanford Natural Language Inference Corpus. 


### Training & Development ###
Up to 5 sentences: how did you evaluate your solution using the dev set before submitting to the leaderboard? What are some key hyperparameter values (e.g., optimizer, learning rate, batch size, etc.)? Did you fine-tune your model or just conducted zero-shot transfer from pre-trained model? If fine-tuning, what portion of data you use and how did you terminate the training (using a fixed #epochs, early stopping based on dev set performance)?

Answer:
I evaluated my solution on dev set using accuracy and loss metric. Some hyperparameters values that I used to fine-tune model are optimizer (AdamW), learning rate (1e-5), batch size (8), epochs (3), weight decay (0.01). I used fixed epoch strategy to terminate my training. I evaluated performance on dev set after every epoch. I used train set to fine-tuning my encoded based transformer model.



### Other methods ###
Did you try other methods than the submitted one?

Answer:
I tried different pretrained models
1. bert-base
2. bert-large
3. roberta-base
4. roberta-large
5. nli-distilroberta-base
6. I also tried passing both precondition and statement sentence embedding which is generated using sentence-bert into neural network to perform binary classification.


### Packages ###
List the key python packages you have used in this assignment.

Answer:
1. numpy
2. pandas
3. scikit-learn
4. pytorch
5. transformers (huggingface)
6. datasets